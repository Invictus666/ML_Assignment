# Machine Learning
========================================================

## Process 

The process of predicting the classe variable is as follows :

a) Extract training data
b) Remove any column with an NA variable. This reduces the number of predictors to 53.
c) We sample 30% of the file into a training dataset.
d) We use the random forest method to create our model.
e) As the number of predictors is large, we preprocess the data using the PCA.

```{r,cache=TRUE}
library(caret)
all_data <- read.csv("pml-training.csv",na.strings=c("NA",""))
NAs <- apply(all_data,2,function(x) {sum(is.na(x))})
all_data <- all_data[,which(NAs == 0)]
set.seed(12345)
in_train <- createDataPartition(y=all_data$classe, p=0.30, list=FALSE)
training <- all_data[in_train,]
testing <- all_data[-in_train,]
removeIndex <- grep("timestamp|X|user_name|new_window",names(training))
training <- training[,-removeIndex]
rfFit <- train(training$classe ~.,data = training,method="rf",prox=TRUE,preProcess="pca")
rfFit
```

Accuracy in-sample is 91.8% which is rather high. 

The out of sample is measured by running the model against the validation data or the testing variable.

```{r}
rfPred <- predict(rfFit, testing)
error_rate <- sum(rfPred == testing$classe)/length(testing$classe)
error_rate
```

The error rate is also high at 94%. We will employ this model on the test data.

```{r}
test_data <- read.csv("pml-testing.csv",na.strings=c("NA",""))
test_pred <- predict(rfFit,test_data)
```

The test_pred variable will be used to create the 20 files to be submitted. This should obtain a score of about 90%+.

